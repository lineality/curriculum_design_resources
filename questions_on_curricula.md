Standard Curriculum and Course Design and Implementation Questions:

As someone with a background in and professional education, STEM education, public and private education, pre-k through 12, night school, municipal and public and private sector institutional adult education experience, including teacher-certification for 2ndary science and product development and management and reporting, including teaching and instruction, experience, and having a long time investment in improving education, here are standard questions that everyone involved in or affected by an educational program should (must) ask: including students, family members, coworkers, teachers, administrators, investors, authors, developers, regulators, publishers, etc. 

- What is reported to whom, when?
- What is the curriculum?
- What is the larger context around this specific course curriculum? (e.g. a degree, a program, institutional goals, etc.)
- What instructional and curricular materials are provided to students and their 'client-system' (family, coworkers etc.)?
- What instructional and curricular materials are not provided to students and their 'client-system' (family, coworkers etc.)? 
- What curriculum standards are being used formally or informally?
- What methodology standards are being used formally or informally?
- What is the syllabus?
- What project-best-practice skills are included or excluded?
- What types and terms for generalization are used in the goals and evaluations for learning? See: https://medium.com/@GeoffreyGordonAshbrook/ai-generalizations-types-testability-2341827faf 
- What are the policies on direct-instruction, automatic-learning, and passive-learning, and literacies?
- What are the policies and mandates relating to projects?
- What are the categories of types of systems of the definitions of what is targeted for (every single item included in) instruction and or evaluation in the curriculum? (e.g. in a context of STEM)
- What tools and processes are available and used in making the curriculum and included materials?
- How is evaluation being done?
- What kinds of evaluation are being done?
- How are evaluation-test-and-feedback processes being evaluated?
- How is evaluation-creation audited?
- What IEP (individualized education plan) vs. course-plan areas of goals, data, tracking, reporting, and remediation are being done?
- How is formative student progress being tracked?
- How is summative student progress being tracked?
- How are pre-post evaluations being carried out in a context of the curriculum's context?
- How are pre-post evaluation data used?
- What official or unofficial additional resources are available to students and their client-system?
- How is student pii(private personally-identifying information) and other private data avoided being collected in the first place or if not avoided how is it being handled? (e.g. are databases and files organized by anonymous numbers, not private information?)
- How are student-cohort statistics being meaningfully anonymized?
- Are and how are effectiveness studies being framed statistically?
- What are the specific goals and prohibitions for what automation
and technology functionalities, features, modules, etc., should, should not, must, and must not do?
- What kind of evaluation mistakes are permissible?
- What kinds of suggestion mistakes are permissible?
- what kinds of automated instruction and initiation (e.g. giving a student an assignment, task, or question) are: should, should not, must, and must not?
- What kind of student-input interactions are: should, should not, must, and must not?
- What kinds of automated responses are: should, should not, must, and must not?
- What access to students have, in present and future, to their evaluations, certifications, etc.?
- What training or prohibitions are in place for using productivity tools before and after learning skills? see: https://medium.com/@GeoffreyGordonAshbrook/should-we-label-images-text-made-by-ai-916df9ac100a 

In a context of statistics, data science, machine learning, AI, there are various categories for specific questions:

1. Are specific models or technologies should, should not, must, and must not? (e.g. 'parametric' models given various contradictory definitions of 'parametric')
1. Does evaluation have a preset outcome, e.g. score distributions or relative ranking?
2. Are the externalization and architectural data aspects in the program?
3. How are ~deterministic (or same-input, same-output) or ~non-deterministic (fuzzy input, fuzzy output) distributed? (note: there is no clear technical term for "same-input, same-output" proverbial determinism, there are many branching areas with different terms). 
4. How are the counting, state, and role issues of generative models mapped over tasks and roles within the course?
5. How are production vs. research performance evaluations done on statistical-data technologies (ancient or modern)? https://medium.com/@GeoffreyGordonAshbrook/lets-test-models-and-let-s-do-tasks-84777f80eb99
6. What hybrid-architectures are used to account for and bridge various categories of types of definitions, exteranlized state items, and structured/unstructured data mosaics? 

See:
- https://medium.com/@GeoffreyGordonAshbrook/ai-generalizations-types-testability-2341827faf
- https://medium.com/@GeoffreyGordonAshbrook/ai-counting-problems-8cb9f66e4c7f 
- https://medium.com/@GeoffreyGordonAshbrook/lets-test-models-and-let-s-do-tasks-84777f80eb99 
- https://medium.com/@GeoffreyGordonAshbrook/should-we-label-images-text-made-by-ai-916df9ac100a 
et al
